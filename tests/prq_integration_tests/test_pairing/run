#!/bin/bash

# You need a working Hadoop cluster to run this.
#set -x
set -o nounset
set -o errexit

TestDir="$(dirname $0)"
source ${TestDir}/../blocks.sh

prep $@

${HADOOP} dfsadmin -safemode wait
${HADOOP} dfs -mkdir "${WD}/bin"
${HADOOP} dfs -put "${TestDir}/input" "${WD}/input"
${HADOOP} jar "${Jar}" -D bl.prq.min-bases-per-read=0 -D bl.prq.drop-failed-filter=false -D mapred.reduce.tasks=4 "${WD}/input" "${WD}/output" 2>/dev/null
${HADOOP} dfs -get "${WD}/output" "${OutputDir}"
${HADOOP} dfs -rmr "${WD}"

python "${TestDir}/check_results.py" "${OutputDir}"/part-*

exit_code=$?

if [ $exit_code == 0 ]; then
	echo 'Test successful!'
else
	echo "Error!  Unexpected result in test ${TestName}" >&2
fi

rm -rf "${OutputDir}"

exit $exit_code
